{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "#from PIL import Image\n",
    "import random\n",
    "#import tensorflow as tf\n",
    "#import re\n",
    "#import datetime\n",
    "#import io\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "#import pickle\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from nltk.metrics.distance import edit_distance\n",
    "#import string\n",
    "#from utils import generate_token_index\n",
    "#from utils import score_prediction, generate_token_index, y_labels, generate_dataset\n",
    "import json\n",
    "import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load filenames and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + 'gt.json') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_orig = dataset['train']\n",
    "dataset_val = dataset['val']\n",
    "dataset_test = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only a subsample for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9340\n"
     ]
    }
   ],
   "source": [
    "#select only a fraction\n",
    "fraction = 0.1\n",
    "max_len = int(len(dataset_train_orig) * fraction)\n",
    "\n",
    "indices = np.random.randint(0, len(dataset_train_orig), max_len)\n",
    "dataset_train = [dataset_train_orig[j] for j in indices]\n",
    "#len(dataset)\n",
    "#dataset = dataset_orig\n",
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, dataset, y_size, x_size, n_channels, batch_size = 16, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.dataset_len = len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        #self.labels = to_categorical(labels)\n",
    "        #self.filenames = filenames\n",
    "        self.y_size = y_size\n",
    "        self.x_size = x_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.max_decoder_seq_length, self.decoder_tokens = self.generate_decoder_tokens()\n",
    "        self.target_token_index, self.num_decoder_tokens = self.generate_token_index()\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.dataset_len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        dataset_temp = [self.dataset[k] for k in indexes]\n",
    "#        labels_temp = [self.labels[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y1, y2 = self.data_generation(dataset_temp)\n",
    "\n",
    "        return [X, y1], y2\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.dataset_len)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def data_generation(self, dataset_temp):        \n",
    "        batch_x = []\n",
    "        batch_y1 = []\n",
    "        batch_y2 = []\n",
    "\n",
    "        for elem in dataset_temp:\n",
    "            #img = cv2.imread(filename, 0)\n",
    "            #resized_image = cv2.resize(img, (self.x_size, self.y_size)) \n",
    "            #normalized_image = resized_image / 255.\n",
    "            \n",
    "            normalized_image = self.preprocess_image(elem[0])\n",
    "            decoder_input_data, decoder_target_data = self.one_hot_labels(elem[1], self.max_decoder_seq_length, \n",
    "                                                                           self.num_decoder_tokens, \n",
    "                                                                           self.target_token_index)\n",
    "\n",
    "            batch_x.append(normalized_image)\n",
    "\n",
    "            batch_y1.append(decoder_input_data)\n",
    "            batch_y2.append(decoder_target_data)\n",
    "        \n",
    "        batch_x = np.asarray(batch_x, dtype = np.float32)\n",
    "\n",
    "#        batch_x = np.asarray(np.reshape(batch_x, (-1, self.y_size, self.x_size, self.n_channels)), dtype = np.float32)\n",
    "        batch_y1 = np.asarray(batch_y1, dtype = np.float32)\n",
    "        batch_y2 = np.asarray(batch_y2, dtype = np.float32)\n",
    "        \n",
    "        return batch_x, batch_y1, batch_y2\n",
    "    \n",
    "    def preprocess_image(self, filename):\n",
    "#        imgs = []\n",
    "#        words = []\n",
    "\n",
    "#        for ind in range(len(dataset)):\n",
    "\n",
    "        #for ind in range(30):\n",
    "        #    print(ind)\n",
    "#            fpath = dataset[ind][1]\n",
    "        #print(file)\n",
    "        img = cv2.imread(filename, 0)\n",
    "        \n",
    "        if img is not None:            \n",
    "            (wt, ht) = self.x_size, self.y_size\n",
    "            (h, w) = img.shape\n",
    "            fx = w / wt\n",
    "            fy = h / ht\n",
    "            f = max(fx, fy)\n",
    "            newSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "            img = cv2.resize(img, newSize)\n",
    "            target = np.ones([ht, wt]) * 255\n",
    "            target[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "            # transpose for TF\n",
    "            #img = cv2.transpose(target)\n",
    "\n",
    "            img = target\n",
    "\n",
    "            # normalize\n",
    "            (m, s) = cv2.meanStdDev(img)\n",
    "            m = m[0][0]\n",
    "            s = s[0][0]\n",
    "            img = img - m\n",
    "            img = img / s if s>0 else img            \n",
    "\n",
    "            img = np.reshape(img, (self.y_size, self.x_size, self.n_channels))\n",
    "            #imgs.append(img)\n",
    "            #words.append(dataset[ind][0])\n",
    "\n",
    "#        array_images = np.asarray(imgs)\n",
    "\n",
    "#        return array_images, words \n",
    "        return img\n",
    "\n",
    "    def generate_decoder_tokens(self):\n",
    "        #target_texts = []\n",
    "        max_decoder_seq_length = 0.\n",
    "        decoder_tokens = set()\n",
    "\n",
    "        for elem in self.dataset:\n",
    "            word = elem[1]\n",
    "        #    word = '[' + word + ']'\n",
    "            decoder_tokens.add('[')\n",
    "            decoder_tokens.add(']')\n",
    "\n",
    "            if len(word) > max_decoder_seq_length:\n",
    "                max_decoder_seq_length = len(word)\n",
    "            for letter in word:\n",
    "                decoder_tokens.add(letter)\n",
    "            #    input_text, target_text = line.split('\\t')\n",
    "            # We use \"tab\" as the \"start sequence\" character\n",
    "            # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "\n",
    "        #    input_texts.append(input_text)\n",
    "        #    target_texts.append(word\n",
    "\n",
    "        max_decoder_seq_length += 2    \n",
    "        decoder_tokens  = list(decoder_tokens)\n",
    "        \n",
    "        return max_decoder_seq_length, decoder_tokens\n",
    "    \n",
    "    def generate_token_index(self):\n",
    "    #decoder_tokens += ':[]'\n",
    "        num_decoder_tokens = len(self.decoder_tokens)\n",
    "\n",
    "        return dict((k, v) for v, k in enumerate(self.decoder_tokens)), num_decoder_tokens\n",
    "    \n",
    "    def one_hot_labels(self, label, max_decoder_seq_length, num_decoder_tokens, target_token_index):\n",
    "\n",
    "#        decoder_input_data = np.zeros( (len(ys),  max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "#        decoder_target_data = np.zeros( (len(ys), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "        decoder_input_data = np.zeros((max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "        decoder_target_data = np.zeros((max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "        \n",
    "#        for i, target_text in enumerate(ys):\n",
    "            \n",
    "        label = '[' + label + ']'\n",
    "\n",
    "        for t, char in enumerate(label):\n",
    "            decoder_input_data[t, target_token_index[char]] = 1.\n",
    "            if t > 0:\n",
    "                decoder_target_data[t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "        return decoder_input_data, decoder_target_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(dataset_train, 128, 384, 1, batch_size = 64)\n",
    "val_generator = DataGenerator(dataset_val, 128, 384, 1, batch_size = 64)\n",
    "\n",
    "#[batch_x, batch_y1], batch_y2 = generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_size = train_generator.y_size\n",
    "x_size = train_generator.x_size\n",
    "max_decoder_seq_length = train_generator.max_decoder_seq_length\n",
    "num_decoder_tokens = train_generator.num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(batch_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = random.randint(0, array_images.shape[0] - 1)\n",
    "plt.figure(figsize=(9, 9)) \n",
    "#plt.imshow(np.reshape(array_images[index, :, :], (y_size, x_size)), cmap=plt.get_cmap('gray'))\n",
    "plt.imshow(batch_x[0], cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "#print(\"word = \", words[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed, Conv2D, MaxPooling2D, Reshape, Dropout, BatchNormalization, Activation, Bidirectional, concatenate, add, Lambda, Permute\n",
    "from keras.callbacks import EarlyStopping\n",
    "#import keras.backend as K\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 512  # Latent dimensionality of the encoding space.\n",
    "#optimizer\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_encoder (InputLayer)      (None, 128, 384, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 128, 384, 16) 784         input_encoder[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_1 (BatchNormalizatio (None, 128, 384, 16) 64          conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 384, 16) 0           batch_norm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_1 (MaxPooling2D)        (None, 64, 192, 16)  0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 64, 192, 32)  12800       maxpool_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_2 (BatchNormalizatio (None, 64, 192, 32)  128         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 192, 32)  0           batch_norm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_2 (MaxPooling2D)        (None, 32, 96, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 32, 96, 64)   51200       maxpool_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_3 (BatchNormalizatio (None, 32, 96, 64)   256         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 96, 64)   0           batch_norm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_3 (MaxPooling2D)        (None, 16, 48, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 16, 48, 128)  73728       maxpool_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_4 (BatchNormalizatio (None, 16, 48, 128)  512         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 48, 128)  0           batch_norm_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_4 (MaxPooling2D)        (None, 8, 24, 128)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 8, 24, 256)   294912      maxpool_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_5 (BatchNormalizatio (None, 8, 24, 256)   1024        conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 24, 256)   0           batch_norm_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_5 (MaxPooling2D)        (None, 4, 12, 256)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, 4, 12, 512)   1179648     maxpool_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_6 (BatchNormalizatio (None, 4, 12, 512)   2048        conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4, 12, 512)   0           batch_norm_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_6 (MaxPooling2D)        (None, 2, 6, 512)    0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 12, 512)      0           maxpool_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_decoder_teacher_forcing ( (None, 23, 80)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_encoder (LSTM)             [(None, 512), (None, 2099200     reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_decoder (LSTM)             [(None, 23, 512), (N 1214464     input_decoder_teacher_forcing[0][\n",
      "                                                                 lstm_encoder[0][1]               \n",
      "                                                                 lstm_encoder[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_dense (TimeDis (None, 23, 80)       41040       lstm_decoder[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,971,808\n",
      "Trainable params: 4,969,792\n",
      "Non-trainable params: 2,016\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(y_size, x_size, 1), name='input_encoder')\n",
    "\n",
    "#encoder_inputs = Input(shape=(128, 384, 1), name='input_encoder')\n",
    "\n",
    "#encoder_inputs = Input(shape=(92, 248, 1), name='input_encoder')\n",
    "\n",
    "#(7,7)\n",
    "encoder_layer = Conv2D(16, (7, 7), padding='same', use_bias=False, name='conv_1')(encoder_inputs)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_1')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_1')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_1')(encoder_layer)\n",
    "\n",
    "#(7,7)\n",
    "encoder_layer = Conv2D(32, (5, 5), padding='same', use_bias=False, name='conv_2')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_2')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_2')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_2')(encoder_layer)\n",
    "\n",
    "#(5,5)\n",
    "encoder_layer = Conv2D(64, (5, 5), padding='same', use_bias=False, name='conv_3')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_3')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_3')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_3')(encoder_layer)\n",
    "\n",
    "#(5,5)\n",
    "encoder_layer = Conv2D(128, (3, 3), padding='same', use_bias=False, name='conv_4')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_4')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_4')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_4')(encoder_layer)\n",
    "\n",
    "#(3,3)\n",
    "encoder_layer = Conv2D(256, (3, 3), padding='same', use_bias=False, name='conv_5')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_5')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_5')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_5')(encoder_layer)\n",
    "\n",
    "#(3,3)\n",
    "encoder_layer = Conv2D(512, (3, 3), padding='same', use_bias=False, name='conv_6')(encoder_layer)\n",
    "encoder_layer = BatchNormalization(name='batch_norm_6')(encoder_layer)\n",
    "encoder_layer = Activation('relu', name='activation_6')(encoder_layer)\n",
    "encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_6')(encoder_layer)\n",
    "\n",
    "\n",
    "#(2,2)\n",
    "#encoder_layer = Conv2D(512, (3, 3), padding='same', use_bias=False, name='conv_6')(encoder_layer)\n",
    "#encoder_layer = BatchNormalization(name='batch_norm_6')(encoder_layer)\n",
    "#encoder_layer = Activation('relu', name='activation_6')(encoder_layer)\n",
    "#encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_6')(encoder_layer)\n",
    "\n",
    "\n",
    "#(3,3)\n",
    "#encoder_layer = Conv2D(512, (3, 3), padding='same', use_bias=False, name='conv_6')(encoder_layer)\n",
    "#encoder_layer = BatchNormalization(name='batch_norm_6')(encoder_layer)\n",
    "#encoder_layer = Activation('relu', name='activation_6')(encoder_layer)\n",
    "#encoder_layer = MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_6')(encoder_layer)\n",
    "\n",
    "conv_shapes = encoder_layer.shape[1:]\n",
    "timesteps = int(conv_shapes[0]*conv_shapes[1])\n",
    "num_features = int(conv_shapes[2])\n",
    "\n",
    "encoder_layer = Reshape((-1, num_features), name='reshape')(encoder_layer)\n",
    "\n",
    "#encoder\n",
    "encoder = LSTM(latent_dim, return_state=True, name='lstm_encoder')\n",
    "_, state_h, state_c = encoder(encoder_layer)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#decoder\n",
    "decoder_inputs = Input(shape=(max_decoder_seq_length, num_decoder_tokens), name='input_decoder_teacher_forcing')\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='lstm_decoder')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense_time_dist = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'), name='time_distributed_dense')\n",
    "decoder_outputs = decoder_dense_time_dist(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "145/145 [==============================] - 41s 283ms/step - loss: 0.4774\n",
      "Epoch 2/50\n",
      "145/145 [==============================] - 39s 270ms/step - loss: 0.4094\n",
      "Epoch 3/50\n",
      "145/145 [==============================] - 39s 269ms/step - loss: 0.3669\n",
      "Epoch 4/50\n",
      "145/145 [==============================] - 39s 269ms/step - loss: 0.3329\n",
      "Epoch 5/50\n",
      "145/145 [==============================] - 39s 269ms/step - loss: 0.3029\n",
      "Epoch 6/50\n",
      "145/145 [==============================] - 39s 271ms/step - loss: 0.2733\n",
      "Epoch 7/50\n",
      "145/145 [==============================] - 39s 268ms/step - loss: 0.2461\n",
      "Epoch 8/50\n",
      "145/145 [==============================] - 39s 269ms/step - loss: 0.2216\n",
      "Epoch 9/50\n",
      "145/145 [==============================] - 39s 269ms/step - loss: 0.1972\n",
      "Epoch 10/50\n",
      "145/145 [==============================] - 39s 266ms/step - loss: 0.1751\n",
      "Epoch 11/50\n",
      "126/145 [=========================>....] - ETA: 5s - loss: 0.1520"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-145eb67e9283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     model.fit_generator(generator=train_generator, epochs=epochs, verbose=1, \n\u001b[0;32m---> 25\u001b[0;31m                         callbacks=[])\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = True\n",
    "\n",
    "#batch_size = 128  # Batch size for training.\n",
    "epochs = 50 # Number of epochs to train for.\n",
    "\n",
    "#n_times_aug = 1\n",
    "\n",
    "# Early stopping  \n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "if train == True:    \n",
    "    # Run training\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "#    model.fit([array_images_train, decoder_input_data_train], decoder_target_data_train, batch_size=batch_size,\n",
    "#              epochs=epochs, validation_data = ([array_images_val, decoder_input_data_val], decoder_target_data_val),\n",
    "#              verbose=1, callbacks=[])\n",
    "\n",
    "    #model.fit_generator(generator = generator epochs=epochs, verbose=1)\n",
    "\n",
    "#    model.fit_generator(generator=train_generator, validation_data=val_generator, epochs=epochs, verbose=1, \n",
    "#                        callbacks=[])\n",
    "\n",
    "    model.fit_generator(generator=train_generator, epochs=epochs, verbose=1, \n",
    "                        callbacks=[])\n",
    "\n",
    "    \n",
    "#    model.fit_generator(train_datagen.flow([array_images_train, decoder_input_data_train], decoder_target_data_train, batch_size=batch_size),\n",
    "#                        steps_per_epoch = n_times_aug * len(array_images_train) / batch_size, epochs=epochs, \n",
    "#                        validation_data = val_datagen.flow([array_images_val, decoder_input_data_val], decoder_target_data_val, batch_size=batch_size),\n",
    "#                        validation_steps =  1, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"../snapshots/graph.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"../snapshots/weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if train == False:\n",
    "#model.save('../snapshots/model_IAM.h5')\n",
    "#model.save_weights(\"../snapshots/weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train == False:\n",
    "    from keras.models import load_model, model_from_json\n",
    "    json_file = open('../snapshots/graph.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"../snapshots/weights.h5\")    \n",
    "#    model = load_model('../snapshots/model_IAM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"../snapshots/weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference models \n",
    "#encoder\n",
    "#encoder_inputs_inference = Input(shape=(x_size, y_size, 1), name='input_encoder_inference')\n",
    "encoder_inference = Model(model.get_input_at(0)[0], model.get_layer(\"lstm_encoder\").output[1:])\n",
    "\n",
    "#decoder\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_inputs_inference = Input(shape=(1, num_decoder_tokens), name='input_decoder_inference')\n",
    "\n",
    "decoder_lstm = model.get_layer(\"lstm_decoder\")\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_inference, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "#dense layer\n",
    "decoder_dense = model.get_layer('time_distributed_dense').layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#inference\n",
    "decoder_inference = Model([decoder_inputs_inference] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq, max_decoder_seq_length=5):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_inference.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['[']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_inference.predict( [target_seq] + states_value )\n",
    "           \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == ']'): \n",
    "            #or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_train = []\n",
    "\n",
    "for ind in range(len(words_train)):    \n",
    "    #row_pred = []\n",
    "\n",
    "    input_seq = np.reshape( array_images_train[ind,:,:], (-1, y_size,  x_size, 1) )\n",
    "    \n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    decoded_sentence = decoded_sentence.replace(\"]\", \"\")\n",
    "    \n",
    "    #row_pred.append(words_train[ind])\n",
    "    #row_pred.append(decoded_sentence)\n",
    "    \n",
    "    pred_train.append(decoded_sentence)\n",
    "    \n",
    "    #print('Data', words_test[ind], '-', ind, 'out of', len(words_test))\n",
    "    print('True sequence:', words_train[ind])\n",
    "    print('Decoded sequence:', decoded_sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test = []\n",
    "\n",
    "for ind in range(len(words_test)):    \n",
    "    #row_pred = []\n",
    "\n",
    "    input_seq = np.reshape( array_images_test[ind,:,:], (-1, y_size,  x_size, 1) )\n",
    "    \n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    decoded_sentence = decoded_sentence.replace(\"]\", \"\")\n",
    "    \n",
    "    #row_pred.append(words_test[ind])\n",
    "    #row_pred.append(decoded_sentence)\n",
    "    \n",
    "    pred_test.append(decoded_sentence)\n",
    "    \n",
    "    #print('Data', words_test[ind], '-', ind, 'out of', len(words_test))\n",
    "    print('True sentence:', words_test[ind])\n",
    "    print('Decoded sentence:', decoded_sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prediction(y_true, y_pred):\n",
    "    words_identified = 0\n",
    "    characters_identified = 0\n",
    "    char_tot = 0\n",
    "#    CER = 0\n",
    "\n",
    "#    list_accuracy_characters = []\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        #pred_row = [y_true[i], y_pred[i]] \n",
    "        \n",
    "        #check if date are the same\n",
    "        #if pred_row[0] == pred_row[1]:\n",
    "        if y_true[i] == y_pred[i]:\n",
    "\n",
    "            words_identified += 1\n",
    "            \n",
    "#        if len(pred_row[1]) < len(pred_row[0]):\n",
    "#            pred_row[1] += '-' * (len(pred_row[0]) - len(pred_row[1]))    \n",
    "#        elif len(pred_row[1]) > len(pred_row[1]):\n",
    "\n",
    "#            pred_row[1] = pred_row[1][0:len(pred_row[0])]\n",
    "\n",
    "        #check the number of characters that are the same\n",
    " #       print(y_true[i])\n",
    " #       print(y_pred[i])\n",
    "        \n",
    "        levenshtein_distance = edit_distance(y_true[i], y_pred[i])\n",
    "        n_char = np.maximum(len(y_true[i]), len(y_pred[i]))\n",
    "        \n",
    "        normalized_distance = levenshtein_distance/n_char\n",
    "\n",
    "        characters_identified += normalized_distance\n",
    "#        char_tot += n_char\n",
    "        \n",
    "#        CER += normalized_distance\n",
    "        \n",
    "#        print(len(y_true[i]))\n",
    "#        for k in range(len(y_true[i])):\n",
    "#            print()\n",
    "#            if y_true[i][k] == y_pred[1][k]:\n",
    "#        characters_identified += 1\n",
    "#        char_tot += 1\n",
    "\n",
    "    # array_accuracy_characters = np.asarray(list_accuracy_characters)\n",
    "    CER = float((characters_identified) / len(y_true))\n",
    "    WER = (len(y_pred) - words_identified)/len(y_pred) \n",
    "        \n",
    "    return CER, WER\n",
    "#    return WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CER_test, WER_test = score_prediction(words_test, pred_test)\n",
    "#WER_test = score_prediction(words_test, pred_test)\n",
    "print('CER: ', round(CER_test * 100, 3), '%')\n",
    "print('WER: ', round(WER_test * 100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CER_train, WER_train = score_prediction(words_train, pred_train)\n",
    "#print('CER: ', round(CER_train * 100, 3), '%')\n",
    "print('CER: ', round(CER_train * 100, 3), '%')\n",
    "print('WER: ', round(WER_train * 100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
